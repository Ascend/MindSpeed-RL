# optimizer/grad offload

## 背景与挑战

在 GRPO、PPO 等 RLHF 算法中，主要耗时会集中在推理阶段，所以通常会使用专门的推理引擎（如 vLLM 等）对推理过程进行加速。 
因此，Actor 模型在训练过程中会同时存在推理态和训练态两种模式，在每轮训练中，Actor 模型需要在训练态和推理态间切换。如何支持训推无缝切换，减少峰值显存占用，提高资源使用效率是一大挑战。

## 解决方案

针对以上问题，本仓库提供训推共卡方案， 具体可见[全共卡/训推共卡介绍](./integrated_worker.md)。通过内存卸载和权重转换技术，在推理阶段将训练相关的优化器状态和梯度卸载至Host侧内存，训练时再将数据重新加载至 NPU 完成更新。

## 使用效果

主要用于训推共卡场景，

1. ​**显著降低峰值显存占用**​：为推理阶段留出充足的 KV Cache 空间，有效降低NPU内存峰值，从根本上避免因内存不足（OOM）导致的训练中断。
2. ​**提升硬件利用与训练效率**​：减少了资源闲置，优化端到端的训练吞吐与效率。

## 使用方法

在训练脚本中加入以下参数配置。
```yaml
generate_config:
  offload_train_optimizer: true     # 设置为 true 可以使能在推理时卸载训练态优化器
  offload_train_grad: true          # 设置为 true 可以使能在推理时卸载训练态梯度
  offload_train_param: true         # 设置为 true 可以使能在推理时卸载训练态权重
```
