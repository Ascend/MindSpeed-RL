

# vLLM 前缀缓存优化

## **功能概述**

前缀缓存（Prefix Caching）是 LLM 推理中广泛采用的 KV 缓存优化技术。其核心思想是​**缓存已处理请求的 KV 计算中间结果**​，当新请求中包含与缓存匹配的相同前缀时，直接复用已缓存的 KV 块，从而避免重复的提示计算。

该机制主要在​**预填充（Prefill）阶段生效**​，对模型输出结果无影响，显著减少计算冗余。

## **使用场景**

前缀缓存已被多数主流 LLM 服务平台（如 OpenAI、Anthropic 等）和开源推理框架（如 SGLang、vLLM 等）广泛采用，尤其适用于：

* 对话型应用中重复或相似的提示前缀
* 批量处理带有共同上下文的请求
* 需要高吞吐、低延迟的在线推理服务

## **使用方法**

在 vLLM 中启用前缀缓存十分简便，只需在生成配置中设置 `enable_prefix_caching` 为 `true`：

```
generate_config:
  enable_prefix_caching: true
```
## **使用效果**

在反复调用同一 Chat 模型或处理相似前缀请求的场景下，启用前缀缓存可带来​**显著的吞吐量提升**​，同时保持生成结果的一致性。

