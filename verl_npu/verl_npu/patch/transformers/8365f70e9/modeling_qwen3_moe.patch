diff --git a/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py b/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py
index d8c216974e..180ece774d 100644
--- a/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py
+++ b/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py
@@ -45,6 +45,10 @@ from ...utils import TransformersKwargs, auto_docstring, can_return_tuple
 from ...utils.deprecation import deprecate_kwarg
 from ...utils.generic import OutputRecorder, check_model_inputs
 from .configuration_qwen3_moe import Qwen3MoeConfig
+from ...utils import is_torch_npu_available
+
+if is_torch_npu_available():
+    import torch_npu
 
 
 def rotate_half(x):
@@ -76,9 +80,9 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
     """
     cos = cos.unsqueeze(unsqueeze_dim)
     sin = sin.unsqueeze(unsqueeze_dim)
-    q_embed = (q * cos) + (rotate_half(q) * sin)
-    k_embed = (k * cos) + (rotate_half(k) * sin)
-    return q_embed, k_embed
+    q_embed = torch_npu.npu_rotary_mul(q, cos, sin)
+    k_embed = torch_npu.npu_rotary_mul(k, cos, sin)
+    return q_embed.to(q.dtype), k_embed.to(k.dtype)
 
 
 def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
@@ -206,13 +210,14 @@ class Qwen3MoeMLP(nn.Module):
         self.act_fn = ACT2FN[config.hidden_act]
 
     def forward(self, x):
-        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
-        return down_proj
+        return self.down_proj(
+            torch_npu.npu_swiglu(torch.cat((self.gate_proj(x), self.up_proj(x)), dim=-1), dim=-1)
+        )
 
 
-class Qwen3MoeSparseMoeBlock(nn.Module):
-    def __init__(self, config):
-        super().__init__()
+class Qwen3MoeSparseMoeBlockPatch(nn.Module):
+    def init(self, config):
+        super().init()
         self.num_experts = config.num_experts
         self.top_k = config.num_experts_per_tok
         self.norm_topk_prob = config.norm_topk_prob
@@ -227,7 +232,6 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
         """ """
         batch_size, sequence_length, hidden_dim = hidden_states.shape
         hidden_states = hidden_states.view(-1, hidden_dim)
-        # router_logits: (batch * sequence_length, n_experts)
         router_logits = self.gate(hidden_states)
 
         routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
@@ -246,21 +250,49 @@ class Qwen3MoeSparseMoeBlock(nn.Module):
         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)
 
         # Loop over all available experts in the model and perform the computation on each expert
-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()
-        for expert_idx in expert_hit:
-            expert_layer = self.experts[expert_idx]
-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))
-
-            # Index the correct hidden states and compute the expert hidden state for
-            # the current expert. We need to make sure to multiply the output hidden
-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)
-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]
-
-            # However `index_add_` only support torch tensors for indexing so we'll use
-            # the `top_x` tensor here.
-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)
+        # Concat all weights
+        input_dtype = hidden_states.dtype
+        up_weight_list = [e.up_proj.weight.t().to(input_dtype) for e in self.experts]
+        gate_weight_list = [e.gate_proj.weight.t().to(input_dtype) for e in self.experts]
+        down_weight_list = [e.down_proj.weight.t().to(input_dtype) for e in self.experts]
+        w1 = torch.stack(up_weight_list)
+        w2 = torch.stack(gate_weight_list)
+        w3 = torch.stack(down_weight_list)
+
+        # Copied from mindspeed moe_utils.py:permute
+        routing_map = selected_experts
+        flatten_indices = routing_map.view(-1)
+        sorted_indices = torch.sort(flatten_indices.float(), stable=True)[1]
+        permuted_tokens = hidden_states.index_select(0, sorted_indices // self.top_k)
+
+        tokens_per_experts = torch.sum(expert_mask, dim=(1, 2))
+        group_list = torch.cumsum(tokens_per_experts, dim=0)
+
+        cpu_group_list = group_list.to("cpu", non_blocking=False)
+        cpu_group_list = [0] + cpu_group_list.tolist()
+        split_size = [cpu_group_list[i + 1] - cpu_group_list[i] for i in range(len(cpu_group_list) - 1)]
+
+        up_res = torch_npu_gmm.apply(permuted_tokens, w1, group_list, split_size)
+        gate_res = torch_npu_gmm.apply(permuted_tokens, w2, group_list, split_size)
+        act_res = torch_npu.npu_swiglu(torch.cat([gate_res, up_res], dim=-1))
+        down_res = torch_npu_gmm.apply(act_res, w3, group_list, split_size)
+
+        probs = routing_weights
+        num_unpermuted_tokens = probs.numel()
+        topk = self.top_k
+        permuted_tokens = down_res
+
+        unpermuted_tokens = torch.zeros(
+            [num_unpermuted_tokens, permuted_tokens.shape[-1]],
+            dtype=permuted_tokens.dtype,
+            device=permuted_tokens.device,
+        )
+        unpermuted_tokens.index_copy_(0, sorted_indices, permuted_tokens)
+        unpermuted_tokens = unpermuted_tokens.reshape(-1, topk, permuted_tokens.size(-1))
+        unpermuted_tokens = unpermuted_tokens * probs.unsqueeze(-1)
+        unpermuted_tokens = unpermuted_tokens.sum(dim=1).to(hidden_states.dtype)
+        final_hidden_states = unpermuted_tokens
+
         return final_hidden_states, router_logits
 
 
@@ -275,11 +307,8 @@ class Qwen3MoeRMSNorm(nn.Module):
         self.variance_epsilon = eps
 
     def forward(self, hidden_states):
-        input_dtype = hidden_states.dtype
-        hidden_states = hidden_states.to(torch.float32)
-        variance = hidden_states.pow(2).mean(-1, keepdim=True)
-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
-        return self.weight * hidden_states.to(input_dtype)
+        return torch_npu.npu_rms_norm(hidden_states, self.weight, epsilon=self.variance_epsilon)[0]
+
 
     def extra_repr(self):
         return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"
