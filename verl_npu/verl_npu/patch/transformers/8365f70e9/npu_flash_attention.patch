diff --git a/src/transformers/integrations/npu_flash_attention.py b/src/transformers/integrations/npu_flash_attention.py
index f4c4e98da9..c5f549c8bd 100644
--- a/src/transformers/integrations/npu_flash_attention.py
+++ b/src/transformers/integrations/npu_flash_attention.py
@@ -19,7 +19,7 @@ from ..utils.import_utils import is_torch_npu_available
 
 
 if is_torch_npu_available():
-    from torch_npu import npu_fusion_attention
+    from torch_npu import npu_fusion_attention, npu_fusion_attention_v2
 
 
 # FlashAttention2 is supported on Ascend NPU with down-right aligned causal mask by default.
@@ -55,6 +55,7 @@ def npu_flash_attn_func(
     dropout_p=0.0,
     softmax_scale=None,
     causal=False,
+    s_aux=None,
     **kwargs,
 ):
     keep_prob = 1.0 - dropout_p
@@ -65,7 +66,7 @@ def npu_flash_attn_func(
     if not causal:
         head_num = q.shape[2]
         output = npu_fusion_attention(q, k, v, head_num, "BSND", keep_prob=keep_prob, scale=softmax_scale)[0]
-    else:
+    elif s_aux is None:
         attn_mask_npu = get_attn_mask_npu(q.device)
         head_num = q.shape[2]
         output = npu_fusion_attention(
@@ -79,6 +80,22 @@ def npu_flash_attn_func(
             atten_mask=attn_mask_npu,
             sparse_mode=SPARSE_MODE,
         )[0]
+    else:
+        sink = s_aux.float()
+        attn_mask_npu = get_attn_mask_npu(q.device)
+        head_num = q.shape[2]
+        output = npu_fusion_attention_v2(
+            q,
+            k,
+            v,
+            head_num,
+            "BSND",
+            keep_prob=keep_prob,
+            scale=softmax_scale,
+            atten_mask=attn_mask_npu,
+            sink=sink,
+            sparse_mode=SPARSE_MODE,
+        )[0]
 
     return output
 
@@ -94,6 +111,7 @@ def npu_flash_attn_varlen_func(
     dropout_p=0.0,
     softmax_scale=None,
     causal=False,
+    s_aux=None,
     **kwargs,
 ):
     keep_prob = 1.0 - dropout_p
@@ -116,7 +134,7 @@ def npu_flash_attn_varlen_func(
             actual_seq_qlen=tuple(cu_seqlens_q[1:].cpu().numpy().tolist()),
             actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),
         )[0]
-    else:
+    elif s_aux is None:
         attn_mask_npu = get_attn_mask_npu(q.device)
         head_num = q.shape[1]
         output = npu_fusion_attention(
@@ -134,5 +152,25 @@ def npu_flash_attn_varlen_func(
             actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),
             sparse_mode=SPARSE_MODE,
         )[0]
+    else:
+        sink = s_aux.float()
+        attn_mask_npu = get_attn_mask_npu(q.device)
+        head_num = q.shape[1]
+        output = npu_fusion_attention_v2(
+            q,
+            k,
+            v,
+            head_num,
+            pse=None,
+            padding_mask=None,
+            atten_mask=attn_mask_npu,
+            scale=softmax_scale,
+            keep_prob=keep_prob,
+            input_layout="TND",
+            sink=sink,
+            actual_seq_qlen=tuple(cu_seqlens_q[1:].cpu().numpy().tolist()),
+            actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),
+            sparse_mode=SPARSE_MODE,
+        )[0]
 
     return output
