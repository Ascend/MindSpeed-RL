diff --git a/verl/trainer/ppo/ray_trainer.py b/verl/trainer/ppo/ray_trainer.py
index 97b68684..1838dcba 100644
--- a/verl/trainer/ppo/ray_trainer.py
+++ b/verl/trainer/ppo/ray_trainer.py
@@ -28,6 +28,7 @@ from dataclasses import dataclass, field
 from enum import Enum
 from pprint import pprint
 from typing import Optional
+import threading
 
 import numpy as np
 import ray
@@ -65,6 +66,42 @@ from verl.utils.tracking import ValidationGenerationsLogger
 WorkerType = type[Worker]
 
 
+class AggregatorActor:
+    def __init__(self, threshold_rate):
+        self.total = 0
+        self.stopped = False
+        self.sample_num = 0
+        self.threshold_rate = threshold_rate
+        self.threshold = 0
+        self.prefetch_request_index_lock = threading.Lock()
+
+    def set_sample_num(self, value):
+        self.sample_num = value
+        self.threshold = int(self.threshold_rate * self.sample_num)
+
+    def clear(self):
+        self.total = 0
+        self.stopped = False
+
+    def add(self, value):
+        with self.prefetch_request_index_lock:
+            self.total += value
+        if self.total >= self.threshold and not self.stopped:
+            self.stopped = True
+        return self.stopped
+
+    def is_stopped(self):
+        return self.stopped
+
+    def get_total(self):
+        return self.total
+
+    def get_threshold(self):
+        return self.threshold
+
+
+
+
 class Role(Enum):
     """
     To create more roles dynamically, you can subclass Role and add new members
@@ -77,6 +114,7 @@ class Role(Enum):
     RefPolicy = 4
     RewardModel = 5
     ActorRolloutRef = 6
+    Aggregator = 7
 
 
 @dataclass
@@ -380,7 +418,8 @@ class RayPPOTrainer:
                 stacklevel=2,
             )
             self.use_critic = False
-
+        self.aggregator = None
+        self.enable_partial_rollout: bool = self.config.algorithm.partial_rollout_max_split > 1
         self._validate_config()
         self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler)
 
@@ -496,7 +535,10 @@ class RayPPOTrainer:
             assert config.actor_rollout_ref.rollout.temperature > 0, (
                 "validation gen temperature should be greater than 0 when enabling do_sample"
             )
-
+        # check partial rollout config
+        assert (config.data.max_response_length %
+                config.algorithm.partial_rollout_max_split == 0), \
+            "max_response_length must be divisible by partial_rollout_max_split"
         print("[validate_config] All configuration checks passed successfully!")
 
     def _create_dataloader(self, train_dataset, val_dataset, collate_fn, train_sampler: Optional[Sampler]):
@@ -787,6 +829,13 @@ class RayPPOTrainer:
         self.resource_pool_manager.create_resource_pool()
 
         self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}
+        
+        if Role.Aggregator in self.role_worker_mapping:
+            self.aggregator = self.role_worker_mapping[Role.Aggregator].options(name="aggregator_actor").remote(
+                threshold_rate=self.config.actor_rollout_ref.rollout.rollout_threshold_rate)
+            self.config.actor_rollout_ref.rollout['aggregator'] = "aggregator_actor"
+
+
 
         # create actor and rollout
         if self.hybrid_engine:
@@ -1080,6 +1129,10 @@ class RayPPOTrainer:
         self.global_steps += 1
         last_val_metrics = None
         self.max_steps_duration = 0
+        
+        
+        partial_batch: Optional[DataProto] = None  # samples whose rollout is not finished yet
+        staged_batch: Optional[DataProto] = None  # samples whose rollout has been finished but not yet trained on
 
         prev_step_profile = False
         curr_step_profile = (
@@ -1102,6 +1155,16 @@ class RayPPOTrainer:
                     )
 
                 batch: DataProto = DataProto.from_single_dict(batch_dict)
+                
+                batch.non_tensor_batch["uid"] = np.array([str(uuid.uuid4()) for _ in range(len(batch.batch))],
+                                                         dtype=object)
+                # repeat to align with repeated responses in rollout
+                batch = batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)
+                batch.non_tensor_batch["age"] = np.ones(len(batch.batch), dtype=int)
+                batch.non_tensor_batch["raw_response_ids"] = np.fromiter(([] for _ in range(len(batch.batch))),
+                                                                         dtype=object)
+
+                batch = DataProto.concat([partial_batch, batch]) if partial_batch is not None else batch
 
                 # add uid to batch
                 batch.non_tensor_batch["uid"] = np.array(
@@ -1110,7 +1173,7 @@ class RayPPOTrainer:
 
                 # pop those keys for generation
                 batch_keys_to_pop = ["input_ids", "attention_mask", "position_ids"]
-                non_tensor_batch_keys_to_pop = ["raw_prompt_ids"]
+                non_tensor_batch_keys_to_pop = ["raw_prompt_ids", "raw_response_ids"]
                 if "multi_modal_data" in batch.non_tensor_batch:
                     non_tensor_batch_keys_to_pop.append("multi_modal_data")
                 if "raw_prompt" in batch.non_tensor_batch:
@@ -1128,7 +1191,7 @@ class RayPPOTrainer:
                     batch_keys=batch_keys_to_pop,
                     non_tensor_batch_keys=non_tensor_batch_keys_to_pop,
                 )
-
+                gen_batch.non_tensor_batch["age"] = batch.non_tensor_batch["age"]
                 # pass global_steps to trace
                 gen_batch.meta_info["global_steps"] = self.global_steps
                 gen_batch = gen_batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)
@@ -1138,13 +1201,77 @@ class RayPPOTrainer:
                 with marked_timer("step", timing_raw):
                     # generate a batch
                     with marked_timer("gen", timing_raw, color="red"):
+                        if self.aggregator:
+                            self.aggregator.set_sample_num.remote(len(gen_batch))
+                            self.aggregator.clear.remote()
                         if not self.async_rollout_mode:
                             gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
                         else:
                             gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch)
                         timing_raw.update(gen_batch_output.meta_info["timing"])
                         gen_batch_output.meta_info.pop("timing", None)
+                    with marked_timer("filter", timing_raw):
+                        batch = batch.union(gen_batch_output)
+
+                        finished_mask = batch.non_tensor_batch.pop("finished")
+                        if self.config.actor_rollout_ref.rollout.partial_rollout_mode == "sync":
+                            finished_mask = (batch.non_tensor_batch[
+                                                 "age"] == self.config.algorithm.partial_rollout_max_split) | finished_mask
+                        if self.config.actor_rollout_ref.rollout.partial_rollout_mode == "async":
+                            finished_mask = ([len(response) >= self.config.actor_rollout_ref.rollout.response_length
+                                              for response in
+                                              gen_batch_output.non_tensor_batch['raw_response_ids']]) | finished_mask
+                        staged_out, partial_batch = DataProto.split_data(batch, finished_mask)
+                        staged_out.non_tensor_batch.pop("raw_prompt_ids")
+                        staged_out.non_tensor_batch.pop("raw_response_ids")
+
+                        if len(partial_batch.batch) > 0:
+                            for key in ("input_ids", "attention_mask", "position_ids"):
+                                tmp = partial_batch.batch.pop(key, None)
+                                partial_batch.batch[key] = tmp[:, : self.config.data.max_prompt_length]
+
+                            for key in ("prompts", "responses", "rollout_log_probs"):
+                                # we don't support rollout_log_probs in this feature branch yet
+                                if key in partial_batch.batch:
+                                    partial_batch.batch.pop(key)
+                        else:
+                            partial_batch = None
+
+                        # note that we no longer ensure the order of samples in staged_batch
+                        staged_batch = DataProto.concat(
+                            [staged_batch, staged_out]) if staged_batch is not None else staged_out
+
+                        # prompts whose number of finished rollout is enough can be trained on
+                        # while filtering, we ensure sample number is divisible by n_gpus_per_node and as large as possible
+                        can_train_mask = np.zeros(len(staged_batch.batch), dtype=bool)
+                        id2count = defaultdict(int)
+                        required_rollouts = self.config.actor_rollout_ref.rollout.n
+                        divisor = self.config.actor_rollout_ref.actor.ppo_mini_batch_size * required_rollouts
+
+                        for uid in staged_batch.non_tensor_batch["uid"]:
+                            id2count[uid] += 1
+                        assert not id2count or max(
+                            id2count.values()) <= required_rollouts, "max number of responses exceeds rollout n"
+
+                        complete_uids = [uid for uid, count in id2count.items() if count == required_rollouts]
 
+                        total_complete_samples = len(complete_uids) * required_rollouts
+                        max_usable_groups = (total_complete_samples // divisor) * divisor // required_rollouts
+                        can_train_count = max_usable_groups * required_rollouts
+
+                        if can_train_count == 0:
+                            print(f"{total_complete_samples=}, no complete uid groups available. Keep generating...")
+                            continue
+
+                        selected_uids = set(complete_uids[:max_usable_groups])
+
+                        for i, uid in enumerate(staged_batch.non_tensor_batch["uid"]):
+                            if uid in selected_uids:
+                                can_train_mask[i] = True
+
+                        batch, staged_batch = DataProto.split_data(staged_batch, can_train_mask)
+                        if partial_batch:
+                            partial_batch.non_tensor_batch["age"] += 1
                     if self.config.algorithm.adv_estimator == AdvantageEstimator.REMAX:
                         if self.reward_fn is None:
                             raise ValueError("A reward_fn is required for REMAX advantage estimation.")
@@ -1166,9 +1293,7 @@ class RayPPOTrainer:
 
                             del gen_baseline_batch, gen_baseline_output
 
-                    # repeat to align with repeated responses in rollout
-                    batch = batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)
-                    batch = batch.union(gen_batch_output)
+
 
                     if "response_mask" not in batch.batch.keys():
                         batch.batch["response_mask"] = compute_response_mask(batch)
@@ -1363,6 +1488,13 @@ class RayPPOTrainer:
                         "training/epoch": epoch,
                     }
                 )
+                if self.enable_partial_rollout:
+                    metrics.update(
+                        {
+                            "training/can_train_count": can_train_count,
+                            "training/total_complete_samples": total_complete_samples,
+                        }
+                    )
                 # collect metrics
                 metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))
                 metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))
