diff --git a/verl/utils/seqlen_balancing.py b/verl/utils/seqlen_balancing.py
index bde116ad..7235da12 100644
--- a/verl/utils/seqlen_balancing.py
+++ b/verl/utils/seqlen_balancing.py
@@ -14,6 +14,7 @@
 
 import copy
 import heapq
+from typing import List
 from itertools import chain
 
 import torch
@@ -147,7 +148,7 @@ def greedy_partition(seqlen_list: list[int], k_partitions: int, equal_size: bool
     return partitions
 
 
-def get_seqlen_balanced_partitions(seqlen_list: list[int], k_partitions: int, equal_size: bool):
+def get_seqlen_balanced_partitions(seqlen_list: list[int], k_partitions: int, equal_size: bool, max_token_len=None):
     """
     Calculates partitions of indices from seqlen_list such that the sum of sequence lengths
     in each partition is balanced. Uses the Karmarkar-Karp differencing method.
@@ -187,7 +188,12 @@ def get_seqlen_balanced_partitions(seqlen_list: list[int], k_partitions: int, eq
         assert seen_idx == set(range(len(seqlen_list)))
         return sorted_partitions
 
-    partitions = karmarkar_karp(seqlen_list=seqlen_list, k_partitions=k_partitions, equal_size=equal_size)
+    if dist.is_initialized() and max_token_len is not None:
+        partitions = heapq_partition_with_max(seqlen_list=seqlen_list, k_partitions=k_partitions, max_token_len=max_token_len)
+        k_partitions = torch.tensor(len(partitions), device=get_device_name())
+        dist.all_reduce(k_partitions, op=dist.ReduceOp.MAX, group=None)
+        k_partitions =  k_partitions.cpu().item()
+    partitions = heapq_partition(seqlen_list=seqlen_list, k_partitions=k_partitions, equal_size=equal_size)
     return _check_and_sort_partitions(partitions)
 
 
@@ -247,6 +253,67 @@ def roundup_divisible(a, b):
     return ((a + b - 1) // b) * b
 
 
+def heapq_partition(seqlen_list: List[int], k_partitions: int, equal_size: bool):
+    equal_part_num = len(seqlen_list) // k_partitions
+
+    sorted_seqlen = sorted([(seqlen, i) for i, seqlen in enumerate(seqlen_list)], reverse=True)
+
+    # Initialize the heap: each group maintains [current sum, number of elements, group index, elements in the group]
+    groups = [[0, 0, i, []] for i in range(k_partitions)]
+    heapq.heapify(groups)
+
+    partitions = []
+    for seqlen, i in sorted_seqlen:
+        current_group = heapq.heappop(groups)
+        current_group[3].append(i)
+        current_group[0] += seqlen
+        current_group[1] += 1
+        if equal_size:
+            if current_group[1] < equal_part_num:
+                heapq.heappush(groups, current_group)
+            else:
+                partitions.append(current_group[3])
+        else:
+            heapq.heappush(groups, current_group)
+
+    partitions.extend([group[3] for group in groups])
+
+    if equal_size:
+        for i, partition in enumerate(partitions):
+            if len(partition) * k_partitions != len(seqlen_list):
+                raise ValueError(f"Partition {i} has {len(partition)} items, expected {len(seqlen_list) // k_partitions}")
+    return partitions
+
+
+def heapq_partition_with_max(seqlen_list: List[int], k_partitions: int, max_token_len: int):
+    # 初始化
+    sorted_seqlen = sorted([(seqlen, i) for i, seqlen in enumerate(seqlen_list)], reverse=True)
+
+    # 初始化堆：每个组维护 [当前和, 元素数量, 组编号, 组内元素]
+    groups = [[0, 0, i, []] for i in range(k_partitions)]
+    group_num = len(groups)
+    heapq.heapify(groups)
+
+    partitions = []
+    for seqlen, i in sorted_seqlen:
+        current_group = heapq.heappop(groups)
+
+        if current_group[0] + seqlen > max_token_len:
+            partitions.append(current_group[3])
+            new_group = [seqlen, 1, group_num, [i]]
+            group_num = group_num + 1
+            heapq.heappush(groups, new_group)
+        else:
+            # 将元素加入该组
+            current_group[0] += seqlen  # 当前组总和增加
+            current_group[1] += 1  # 当前组元素数量加1
+            current_group[3].append(i)  # 当前组加入元素
+            # 如果未满员，重新放回堆中
+            heapq.heappush(groups, current_group)
+    partitions.extend([group[3] for group in groups])
+    return partitions
+
+
 def rearrange_micro_batches(
     batch,
     max_token_len,
@@ -294,7 +361,7 @@ def rearrange_micro_batches(
     seq_len_effective = seq_len_effective.tolist()
     assert num_micro_batches <= len(seq_len_effective)
 
-    micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False)
+    micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False, max_token_len=max_token_len)
 
     if use_dynamic_bsz_balance:
         # Use the sum of squared sequence lengths to approximate attention computation workload
