diff --git a/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py b/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
index 23eb3346..59c63dd5 100644
--- a/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
+++ b/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
@@ -29,6 +29,8 @@ When working with Megatron:
 import getpass
 import logging
 import os
+import uuid
+import copy
 import pickle
 import socket
 import threading
@@ -46,19 +48,23 @@ from filelock import FileLock
 from omegaconf import DictConfig, OmegaConf
 from tensordict import TensorDict
 from vllm import LLM, SamplingParams
+import vllm.envs as envs
 from vllm.distributed import parallel_state as vllm_ps
 from vllm.lora.request import LoRARequest
 from vllm.model_executor.sampling_metadata import SamplingMetadata
 from vllm.worker.worker_base import WorkerWrapperBase
+from vllm.distributed import parallel_state as vpu

 from verl import DataProto
 from verl.utils.profiler import GPUMemoryLogger
 from verl.utils.torch_functional import get_response_mask, pad_2d_list_to_length
 from verl.workers.rollout.base import BaseRollout
+from verl.workers.sharding_manager.hybrid_tp_config import HybridTPConfig

 logger = logging.getLogger(__file__)
 logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))

+
 # TODO
 # 1. support pp in vllm
 # 2. passing tokenizer is not necessary? no encoding/decoding is happending here
@@ -89,12 +95,22 @@ class vLLMRollout(BaseRollout):
         super().__init__()
         self.config = config

+        # create HybridTPConfig
+        self.hybrid_tp_config = HybridTPConfig.from_dict_config(
+            self.config.get("hybrid_tp", {}),
+        )
+
+        print(f"[NPU Patch] hybrid_tp_config is : {self.hybrid_tp_config if self.hybrid_tp_config else '{}'}")
+
         tensor_parallel_size = self.config.get("tensor_model_parallel_size", 1)
         assert tensor_parallel_size <= torch.distributed.get_world_size(), (
             "tensor parallel size should be less than or equal to the world size"
         )
         max_num_batched_tokens = self.config.get("max_num_batched_tokens", 8192)
-
+        if self.config["aggregator"]:
+            self.aggregator = ray.get_actor("aggregator_actor")
+        else:
+            self.aggregator = None
         if kwargs.get("train_tp") is not None:
             # deployed with megatron
             import os
@@ -138,10 +154,10 @@ class vLLMRollout(BaseRollout):

         max_model_len = int(config.max_model_len or config.prompt_length + config.response_length)

-        if max_num_batched_tokens < max_model_len and self.config.enable_chunked_prefill:
+        if max_num_batched_tokens < max_model_len and not self.config.enable_chunked_prefill:
             raise ValueError(
-                "Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len, \
-                             please increase max_num_batched_tokens or disable chunked prefill"
+                "max_num_batched_tokens is smaller than max_model_len, \
+                             please increase max_num_batched_tokens or enable chunked prefill"
             )

         trust_remote_code = kwargs.get("trust_remote_code", False)
@@ -162,6 +178,26 @@ class vLLMRollout(BaseRollout):
         engine_kwargs = {key: val for key, val in engine_kwargs.items() if val is not None}
         if config.get("limit_images", None):  # support for multi-image data
             engine_kwargs["limit_mm_per_prompt"] = {"image": config.get("limit_images")}
+        # patch this for npu
+        if hasattr(config, "dp_model_parallel_size") and config.dp_model_parallel_size > 1:
+            self._init_dp_env(config)
+
+        # Extract hybrid TP config for additional_config
+        additional_config = {}
+        if self.hybrid_tp_config.enabled:
+            # Extract tp_size values from hybrid_tp_config
+            if self.hybrid_tp_config.qkv_proj_tp_size is not None:
+                additional_config["qkvproj_tensor_parallel_size"] = self.hybrid_tp_config.qkv_proj_tp_size
+            if self.hybrid_tp_config.o_proj_tp_size is not None:
+                additional_config["oproj_tensor_parallel_size"] = self.hybrid_tp_config.o_proj_tp_size
+            if self.hybrid_tp_config.lm_head_tp_size is not None:
+                additional_config["lmhead_tensor_parallel_size"] = self.hybrid_tp_config.lm_head_tp_size
+
+        print(f"[NPU Patch] vLLM additional_config: {additional_config if additional_config else '{}'}")
+
+        # Add additional_config to engine_kwargs if not empty
+        if additional_config:
+            engine_kwargs["additional_config"] = additional_config

         self.inference_engine = LLM(
             model=model_path,
@@ -178,8 +214,11 @@ class vLLMRollout(BaseRollout):
             disable_log_stats=config.disable_log_stats,
             max_num_batched_tokens=max_num_batched_tokens,
             enable_chunked_prefill=config.enable_chunked_prefill,
-            enable_prefix_caching=True,
+            enable_prefix_caching=config.get("enable_prefix_caching", True),
             trust_remote_code=trust_remote_code,
+            enable_expert_parallel=config.get("enable_expert_parallel", False),
+            max_num_seqs=int(config.get("max_num_seqs", 256)),
+            compilation_config={"cudagraph_capture_sizes": [8, 16, 32, 64, 128, 192, 256, 384]},
             seed=config.get("seed", 0),
             **lora_kwargs,
             **engine_kwargs,
@@ -207,6 +246,36 @@ class vLLMRollout(BaseRollout):

         self.pad_token_id = tokenizer.pad_token_id

+    def _init_dp_env(self, config):
+        rank = torch.distributed.get_rank()
+
+        world_size = torch.distributed.get_world_size()
+        tp_size = int(config.get("tensor_model_parallel_size", 1))
+        dp_size = int(config.get("dp_model_parallel_size", 1))
+
+        all_ranks = torch.arange(world_size).reshape(-1, dp_size, 1, tp_size)  # noqa
+        group_ranks = all_ranks.transpose(1, 3).reshape(-1, dp_size).unbind(0)
+        group_ranks = [x.tolist() for x in group_ranks]
+
+        # all gather ip
+        ip_addr = ray.util.get_node_ip_address()
+        ip_list = [None] * world_size
+        torch.distributed.all_gather_object(ip_list, ip_addr)
+
+        for index, group_rank in enumerate(group_ranks):
+            if torch.distributed.get_rank() in group_rank:
+                os.environ["VLLM_DP_MASTER_PORT"] = str(int(os.environ.get("MASTER_PORT")) + 1 + index)
+                os.environ["VLLM_DP_MASTER_IP"] = ip_list[group_rank[0]]
+            local_dp_rank = rank // tp_size % dp_size
+            os.environ["VLLM_DP_RANK"] = str(local_dp_rank)
+            os.environ["VLLM_DP_SIZE"] = str(dp_size)
+            os.environ["VLLM_PORT"] = os.environ["VLLM_DP_MASTER_PORT"]
+            envs.VLLM_DP_RANK = int(os.environ["VLLM_DP_RANK"])
+            envs.VLLM_DP_MASTER_IP = os.environ["VLLM_DP_MASTER_IP"]
+            envs.VLLM_DP_MASTER_PORT = int(os.environ["VLLM_DP_MASTER_PORT"])
+
+            print(f"[VLLM] using TP={tp_size}, DP={dp_size}", flush=True)
+
     @contextmanager
     def update_sampling_params(self, **kwargs):
         # update sampling params
@@ -257,6 +326,9 @@ class vLLMRollout(BaseRollout):
         batch_size = idx.size(0)

         non_tensor_batch = prompts.non_tensor_batch
+
+        age_list = non_tensor_batch["age"] if "age" in non_tensor_batch else None
+        index_list = [i for i, val in sorted(enumerate(age_list), key=lambda x: (-x[1], x[0]))]
         if "raw_prompt_ids" not in non_tensor_batch:
             non_tensor_batch["raw_prompt_ids"] = np.array(
                 [_pre_process_inputs(self.pad_token_id, idx[i]) for i in range(batch_size)], dtype=object
@@ -265,6 +337,12 @@ class vLLMRollout(BaseRollout):
         if batch_size != len(non_tensor_batch["raw_prompt_ids"]):
             raise RuntimeError("vllm sharding manager is not work properly.")

+        raw_prompt_ids = non_tensor_batch.pop("raw_prompt_ids")
+        if "raw_response_ids" in non_tensor_batch and self.config.partial_rollout_max_split > 0:
+            raw_response_ids = non_tensor_batch.pop("raw_response_ids")
+        else:
+            raw_response_ids = np.fromiter(([] for _ in range(batch_size)), dtype=object)
+
         if "multi_modal_data" in non_tensor_batch:
             vllm_inputs = []
             for raw_prompt_ids, multi_modal_data in zip(
@@ -272,9 +350,10 @@ class vLLMRollout(BaseRollout):
             ):
                 vllm_inputs.append({"prompt_token_ids": raw_prompt_ids, "multi_modal_data": multi_modal_data})
         else:
-            vllm_inputs = [
-                {"prompt_token_ids": raw_prompt_ids} for raw_prompt_ids in non_tensor_batch.pop("raw_prompt_ids")
-            ]
+            vllm_inputs = [{"prompt_token_ids": raw_prompt_ids_ + raw_response_ids_} for
+                           raw_prompt_ids_, raw_response_ids_ in zip(raw_prompt_ids, raw_response_ids)]
+
+        need_response_length = [self.config.response_length - len(response) for response in raw_response_ids]

         for input_data in vllm_inputs:
             # Ensure token IDs are lists or numpy arrays
@@ -304,6 +383,11 @@ class vLLMRollout(BaseRollout):
                 "temperature": self.config.val_kwargs.temperature,
                 "n": 1,  # if validate, already repeat in ray_trainer
             }
+        elif self.config.partial_rollout_mode == "sync" and self.config.partial_rollout_max_split > 0:
+            kwargs = {
+                "n": 1,  # also repeated in ray_trainer
+                "max_tokens": self.config.response_length // self.config.partial_rollout_max_split,
+            }

         lora_requests = None
         if self.lora_kwargs:
@@ -316,27 +400,48 @@ class vLLMRollout(BaseRollout):

         # users can customize different sampling_params at different run
         with self.update_sampling_params(**kwargs):
-            outputs = self.inference_engine.generate(
-                prompts=vllm_inputs,  # because we have already convert it to prompt token id
-                sampling_params=self.sampling_params,
-                lora_request=lora_requests,
-                use_tqdm=False,
-            )
+            if self.config.partial_rollout_mode == "sync" or self.config.partial_rollout_max_split == -1:
+                outputs = self.inference_engine.generate(
+                    prompts=vllm_inputs,  # because we have already convert it to prompt token id
+                    sampling_params=self.sampling_params,
+                    lora_request=lora_requests,
+                    use_tqdm=False,
+                )
+            else:
+                new_vllm_inputs = [vllm_inputs[i] for i in index_list]
+                outputs_sorted = self.async_generate_sequences(
+                    prompts=new_vllm_inputs,  # because we have already convert it to prompt token id
+                    sampling_params=self.sampling_params,
+                    need_response_length=need_response_length,
+                    age_list=age_list
+                )
+                reverse_index = {j: k for k, j in enumerate(index_list)}
+                outputs = [outputs_sorted[reverse_index[j]] for j in range(len(index_list))]

             # TODO(sgm): disable logprob when recompute_log_prob is enable
             # if n = 1: (bs, response_length) ; if n > 1: (bs * n, response_length)

             response = []
+            finished = []
             rollout_log_probs = []
             for output in outputs:
                 for sample_id in range(len(output.outputs)):
                     response_ids = output.outputs[sample_id].token_ids
                     response.append(response_ids)
+                    if self.config.partial_rollout_mode == "sync":
+                        finished.append(output.outputs[sample_id].finish_reason != "length")
+                    else:
+                        finished.append(output.finished == True)
                     if self.config.calculate_log_probs:
                         curr_log_prob = []
                         for i, logprob in enumerate(output.outputs[sample_id].logprobs):
                             curr_log_prob.append(logprob[response_ids[i]].logprob)
                         rollout_log_probs.append(curr_log_prob)
+                        non_tensor_batch["finished"] = np.array(finished)
+            non_tensor_batch["finished"] = np.array(finished)
+            response = raw_response_ids + np.fromiter(response, dtype=object)
+            non_tensor_batch["raw_response_ids"] = response
+            non_tensor_batch["raw_prompt_ids"] = raw_prompt_ids

             response = pad_2d_list_to_length(response, self.pad_token_id, max_length=self.config.response_length).to(
                 idx.device
@@ -383,6 +488,67 @@ class vLLMRollout(BaseRollout):

         return DataProto(batch=batch, non_tensor_batch=non_tensor_batch)

+    def async_generate_sequences(self, prompts: list, **kwargs):
+
+        engine = self.inference_engine.llm_engine
+
+        is_pipeline_last_stage = vpu.get_pipeline_model_parallel_group().is_last_rank
+
+        need_response_length = kwargs.pop("need_response_length")
+        age_list = kwargs.pop("age_list")
+
+        if self.aggregator:
+            aged_sample_num = sum(age_list == self.config.partial_rollout_max_split)
+        else:
+            aged_sample_num = 0
+            samples_num = len(prompts)
+
+        stop_signal = None
+        output_list = []
+        for i, prompt_token_ids in enumerate(prompts):
+            request_id = f"req_{i}_{uuid.uuid4().hex[:6]}"
+
+            sampling_params = copy.deepcopy(self.sampling_params)
+            sampling_params.max_tokens = need_response_length[i]
+            engine.add_request(
+                request_id=request_id,
+                prompt=prompt_token_ids,
+                params=sampling_params
+            )
+        count = 0
+
+        t = 0
+        while engine.has_unfinished_requests():
+            t = t + 1
+            step_outputs = engine.step()
+            if self.aggregator:
+                if ray.get(self.aggregator.is_stopped.remote()) and count >= aged_sample_num and t % 20 == 0:
+                    stop_signal = True
+
+            else:
+                if len(output_list) >= samples_num and count >= aged_sample_num and t % 20 == 0:
+                    stop_signal = True
+
+            for output in step_outputs:
+                if output.finished or stop_signal:
+
+                    if age_list is not None:
+                        aged_prompt = age_list[
+                                          int(output.request_id.split("_")[1])] == self.config.partial_rollout_max_split
+                        if aged_prompt:
+                            count += 1
+                    output_list.append(output)
+                    request_id = output.request_id
+
+                    if self.aggregator:
+                        if is_pipeline_last_stage and vpu.get_tensor_model_parallel_rank() == 0:
+                            self.aggregator.add.remote(1)
+                if stop_signal:
+                    engine.abort_request([request_id])
+        output_list.sort(key=lambda x: int(x.request_id.split("_")[1]))
+
+        return output_list
+

 # https://github.com/vllm-project/vllm/issues/13175
 def _monkey_patch_compute_logits(model, vocab_size: int):
