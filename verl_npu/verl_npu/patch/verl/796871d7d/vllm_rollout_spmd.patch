diff --git a/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py b/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
index 23eb3346..2074932e 100644
--- a/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
+++ b/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
@@ -46,6 +46,7 @@ from filelock import FileLock
 from omegaconf import DictConfig, OmegaConf
 from tensordict import TensorDict
 from vllm import LLM, SamplingParams
+import vllm.envs as envs
 from vllm.distributed import parallel_state as vllm_ps
 from vllm.lora.request import LoRARequest
 from vllm.model_executor.sampling_metadata import SamplingMetadata
@@ -55,6 +56,7 @@ from verl import DataProto
 from verl.utils.profiler import GPUMemoryLogger
 from verl.utils.torch_functional import get_response_mask, pad_2d_list_to_length
 from verl.workers.rollout.base import BaseRollout
+from verl.workers.sharding_manager.hybrid_tp_config import HybridTPConfig
 
 logger = logging.getLogger(__file__)
 logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))
@@ -89,6 +91,13 @@ class vLLMRollout(BaseRollout):
         super().__init__()
         self.config = config
 
+        # create HybridTPConfig
+        self.hybrid_tp_config = HybridTPConfig.from_dict_config(
+            self.config.get("hybrid_tp", {}),
+        )
+
+        print(f"[NPU Patch] hybrid_tp_config is : {self.hybrid_tp_config if self.hybrid_tp_config else '{}'}")
+
         tensor_parallel_size = self.config.get("tensor_model_parallel_size", 1)
         assert tensor_parallel_size <= torch.distributed.get_world_size(), (
             "tensor parallel size should be less than or equal to the world size"
@@ -162,6 +171,26 @@ class vLLMRollout(BaseRollout):
         engine_kwargs = {key: val for key, val in engine_kwargs.items() if val is not None}
         if config.get("limit_images", None):  # support for multi-image data
             engine_kwargs["limit_mm_per_prompt"] = {"image": config.get("limit_images")}
+        # patch this for npu
+        if hasattr(config, "dp_model_parallel_size") and config.dp_model_parallel_size > 1:
+            self._init_dp_env(config)
+
+        # Extract hybrid TP config for additional_config
+        additional_config = {}
+        if self.hybrid_tp_config.enabled:
+            # Extract tp_size values from hybrid_tp_config
+            if self.hybrid_tp_config.qkv_proj_tp_size is not None:
+                additional_config["qkvproj_tensor_parallel_size"] = self.hybrid_tp_config.qkv_proj_tp_size
+            if self.hybrid_tp_config.o_proj_tp_size is not None:
+                additional_config["oproj_tensor_parallel_size"] = self.hybrid_tp_config.o_proj_tp_size
+            if self.hybrid_tp_config.lm_head_tp_size is not None:
+                additional_config["lmhead_tensor_parallel_size"] = self.hybrid_tp_config.lm_head_tp_size
+
+        print(f"[NPU Patch] vLLM additional_config: {additional_config if additional_config else '{}'}")
+
+        # Add additional_config to engine_kwargs if not empty
+        if additional_config:
+            engine_kwargs["additional_config"] = additional_config
 
         self.inference_engine = LLM(
             model=model_path,
@@ -178,8 +207,11 @@ class vLLMRollout(BaseRollout):
             disable_log_stats=config.disable_log_stats,
             max_num_batched_tokens=max_num_batched_tokens,
             enable_chunked_prefill=config.enable_chunked_prefill,
-            enable_prefix_caching=True,
+            enable_prefix_caching=config.get("enable_prefix_caching", True),
             trust_remote_code=trust_remote_code,
+            enable_expert_parallel=config.get("enable_expert_parallel", False),
+            max_num_seqs=int(config.get("max_num_seqs", 256)),
+            compilation_config={"cudagraph_capture_sizes": [8, 16, 32, 64, 128, 192, 256, 384]},
             seed=config.get("seed", 0),
             **lora_kwargs,
             **engine_kwargs,
@@ -207,6 +239,37 @@ class vLLMRollout(BaseRollout):
 
         self.pad_token_id = tokenizer.pad_token_id
 
+
+    def _init_dp_env(self, config):
+        rank = torch.distributed.get_rank()
+        world_size = torch.distributed.get_world_size()
+        tp_size = int(config.get("tensor_model_parallel_size", 1))
+        dp_size = int(config.get("dp_model_parallel_size", 1))
+
+        all_ranks = torch.arange(world_size).reshape(-1, dp_size, 1, tp_size)  # noqa
+        group_ranks = all_ranks.transpose(1, 3).reshape(-1, dp_size).unbind(0)
+        group_ranks = [x.tolist() for x in group_ranks]
+
+        # all gather ip
+        ip_addr = ray.util.get_node_ip_address()
+        ip_list = [None] * world_size
+        torch.distributed.all_gather_object(ip_list, ip_addr)
+
+        for index, group_rank in enumerate(group_ranks):
+            if torch.distributed.get_rank() in group_rank:
+                os.environ["VLLM_DP_MASTER_PORT"] = str(int(os.environ.get("MASTER_PORT")) + 1 + index)
+                os.environ["VLLM_DP_MASTER_IP"] = ip_list[group_rank[0]]
+        local_dp_rank = rank // tp_size % dp_size
+        os.environ["VLLM_DP_RANK"] = str(local_dp_rank)
+        os.environ["VLLM_DP_SIZE"] = str(dp_size)
+        os.environ["VLLM_PORT"] = os.environ["VLLM_DP_MASTER_PORT"]
+        envs.VLLM_DP_RANK = int(os.environ["VLLM_DP_RANK"])
+        envs.VLLM_DP_MASTER_IP = os.environ["VLLM_DP_MASTER_IP"]
+        envs.VLLM_DP_MASTER_PORT = int(os.environ["VLLM_DP_MASTER_PORT"])
+
+        print(f"[VLLM] using TP={tp_size}, DP={dp_size}", flush=True)
+
+
     @contextmanager
     def update_sampling_params(self, **kwargs):
         # update sampling params