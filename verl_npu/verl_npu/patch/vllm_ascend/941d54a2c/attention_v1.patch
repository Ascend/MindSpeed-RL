diff --git a/vllm_ascend/attention/attention_v1.py b/vllm_ascend/attention/attention_v1.py
index 846f68db..47bc1aa9 100644
--- a/vllm_ascend/attention/attention_v1.py
+++ b/vllm_ascend/attention/attention_v1.py
@@ -45,7 +45,7 @@ from vllm_ascend.ops.attention import vanilla_chunked_prefill
 from vllm_ascend.utils import (ACL_FORMAT_FRACTAL_NZ, aligned_16, is_310p,
                                nd_to_nz_2d, nd_to_nz_spec,
                                prefill_context_parallel_enable,
-                               weak_ref_tensors)
+                               weak_ref_tensors, set_use_sink_attention)
 
 # isort: off
 if prefill_context_parallel_enable():
@@ -539,6 +539,7 @@ class AscendAttentionBackendImpl(AttentionImpl):
         logits_soft_cap: Optional[float],
         attn_type: str,
         kv_sharing_target_layer_name: Optional[str],
+        sinks: torch.Tensor = None,
         **kwargs,
     ) -> None:
         self.num_heads = num_heads
@@ -559,6 +560,20 @@ class AscendAttentionBackendImpl(AttentionImpl):
         self.num_queries_per_kv = self.num_heads // self.num_kv_heads
         self.key_cache = None
         self.value_cache = None
+        self.share_mask_tril_spase = None
+        if self.sliding_window is not None:
+            mask = torch.ones(2048, 2048, dtype=torch.bool)
+            triu_mask = torch.triu(mask, diagonal=1).to("npu")
+            tril_mask = torch.tril(mask, -self.sliding_window).to("npu")
+            self.share_mask_tril_spase = triu_mask + tril_mask
+        else:
+            self.share_mask_tril_spase = ~torch.tril(torch.ones((2048, 2048), device='npu', dtype=torch.bool))
+
+        # For sink attention
+        self.sinks = sinks
+        
+        set_use_sink_attention(self.sinks is not None)
+
         self.pcp_size = get_prefill_context_model_parallel_world_size(
         ) if prefill_context_parallel_enable() else 1
         self.pcp_rank = get_prefill_context_model_parallel_rank(
@@ -709,15 +724,40 @@ class AscendAttentionBackendImpl(AttentionImpl):
             mask = torch_npu.npu_format_cast(mask.contiguous(),
                                              ACL_FORMAT_FRACTAL_NZ)
 
-        torch_npu._npu_flash_attention(query=query,
-                                       key=key,
-                                       value=value,
-                                       mask=mask,
-                                       seq_len=attn_metadata.seq_lens,
-                                       scale_value=self.scale,
-                                       num_heads=self.num_heads,
-                                       num_kv_heads=self.num_kv_heads,
-                                       out=output)
+        if self.sinks is not None:
+            if self.sliding_window is not None:
+                sparse_mode=4
+            else:
+                sparse_mode=3
+            actual_seq_qlen = attn_metadata.query_lens.cumsum(dim=0)
+            actual_seq_kvlen = attn_metadata.seq_lens.cumsum(dim=0)
+            attn_output, _ = torch_npu.npu_fused_infer_attention_score_v2(
+                query[:actual_seq_qlen[-1], :],
+                key[:actual_seq_qlen[-1], :],
+                value[:actual_seq_qlen[-1], :],
+                num_query_heads=self.num_heads,
+                num_key_value_heads=self.num_kv_heads,
+                input_layout="TND",
+                pre_tokens=self.sliding_window if self.sliding_window is not None else 2147483647,
+                next_tokens=0,
+                atten_mask=self.share_mask_tril_spase,
+                sparse_mode=sparse_mode,
+                softmax_scale=self.scale,
+                actual_seq_qlen=actual_seq_qlen,
+                actual_seq_kvlen=actual_seq_kvlen,
+                learnable_sink=self.sinks,
+            )
+            output[:actual_seq_qlen[-1], :].copy_(attn_output)
+        else:
+            torch_npu._npu_flash_attention(query=query,
+                                        key=key,
+                                        value=value,
+                                        mask=mask,
+                                        seq_len=attn_metadata.seq_lens,
+                                        scale_value=self.scale,
+                                        num_heads=self.num_heads,
+                                        num_kv_heads=self.num_kv_heads,
+                                        out=output)
         assert output is not None
         return output[:num_tokens]
 
@@ -734,8 +774,33 @@ class AscendAttentionBackendImpl(AttentionImpl):
         batch_size = attn_metadata.query_lens.shape[0]
         block_table = attn_metadata.block_tables[:batch_size, :]
         num_block, block_size, _, _ = self.key_cache.shape  # type: ignore
-
-        if block_size == 128:
+        if self.sinks is not None:
+            if self.sliding_window is not None:
+                sparse_mode=4
+            else:
+                sparse_mode=3
+            num_block, block_size, _, _ = self.key_cache.shape
+            actual_seq_qlen = attn_metadata.query_lens.cumsum(dim=0)
+            attn_output, _ = torch_npu.npu_fused_infer_attention_score_v2(
+                query[:actual_seq_qlen[-1], :],
+                self.key_cache.view(num_block, block_size, self.num_kv_heads * self.head_size),
+                self.value_cache.view(num_block, block_size, self.num_kv_heads * self.head_size),
+                num_query_heads=self.num_heads,
+                num_key_value_heads=self.num_kv_heads,
+                input_layout="TND",
+                pre_tokens=self.sliding_window if self.sliding_window is not None else 2147483647,
+                next_tokens=0,
+                atten_mask=self.share_mask_tril_spase,
+                sparse_mode=sparse_mode,
+                softmax_scale=self.scale,
+                block_table=block_table,
+                block_size=block_size,
+                actual_seq_qlen=actual_seq_qlen,
+                actual_seq_kvlen=attn_metadata.seq_lens,
+                learnable_sink=self.sinks,
+                )
+            output[:actual_seq_qlen[-1], :].copy_(attn_output)
+        elif block_size == 128:
             # TODO:The npu_fused_infer_attention_score op is planned to
             # be utilized in a wider range in upcoming versions.
             key = self.key_cache.view(  # type: ignore
@@ -783,7 +848,32 @@ class AscendAttentionBackendImpl(AttentionImpl):
             # seq_lens_tensor needs to be transferred to the device for 310P.
             attn_metadata.seq_lens = \
                 attn_metadata.seq_lens.to(device=query.device)
-        if self.sliding_window is not None and attn_metadata.seq_lens.shape[
+        if self.sinks is not None:
+            if self.sliding_window is not None:
+                sparse_mode=4
+            else:
+                sparse_mode=3
+            num_block, block_size, _, _ = self.key_cache.shape  # type: ignore
+            actual_seq_qlen = torch.tensor([1] * len(attn_metadata.seq_lens_list), dtype=torch.int32).cumsum(dim=0)
+            output, _ = torch_npu.npu_fused_infer_attention_score_v2(
+                    query,
+                    self.key_cache.view(num_block, block_size, self.num_kv_heads * self.head_size),
+                    self.value_cache.view(num_block, block_size, self.num_kv_heads * self.head_size),
+                    num_query_heads=self.num_heads,
+                    num_key_value_heads=self.num_kv_heads,
+                    input_layout="TND",
+                    pre_tokens=self.sliding_window if self.sliding_window is not None else 2147483647,
+                    next_tokens=0,
+                    atten_mask=self.share_mask_tril_spase,
+                    sparse_mode=sparse_mode,
+                    softmax_scale=self.scale,
+                    block_table=attn_metadata.block_tables,
+                    block_size=block_size,
+                    actual_seq_qlen=actual_seq_qlen,
+                    actual_seq_kvlen=attn_metadata.seq_lens_list,
+                    learnable_sink=self.sinks,
+                )
+        elif self.sliding_window is not None and attn_metadata.seq_lens.shape[
                 0] == query.size(0):
             batch_size = attn_metadata.seq_lens.shape[0]
             block_size = 128
@@ -829,61 +919,86 @@ class AscendAttentionBackendImpl(AttentionImpl):
         attn_metadata: AscendMetadata,
         output: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
-        # Use chunked prefill for head size 192 scenario, like deepseek
-        # paged_attention_splitfuse maybe crash at such scenario.
-        # TODO: vanilla path will be removed after the kernel support
-        # head_size 192 scenario.
-        if self.head_size == 192:
-            cu_seqlen_q = [0] + attn_metadata.query_lens.tolist()
-            cu_seqlen_k = [0] + attn_metadata.seq_lens.tolist()
-            cu_seqlen_q = torch.tensor(cu_seqlen_q, device=query.device)
-            cu_seqlen_k = torch.tensor(cu_seqlen_k, device=query.device)
-            cu_seqlen_q = torch.cumsum(cu_seqlen_q, dim=0)
-            cu_seqlen_k = torch.cumsum(cu_seqlen_k, dim=0)
-            max_seqlen_q = torch.max(attn_metadata.query_lens)
-            max_seqlen_k = torch.max(attn_metadata.seq_lens)
-            vanilla_chunked_prefill(output, query, self.key_cache,
-                                    self.value_cache,
-                                    attn_metadata.block_tables, cu_seqlen_q,
-                                    cu_seqlen_k, max_seqlen_q, max_seqlen_k,
-                                    self.scale, None, True)
-            return output
-
-        # Use paged attention.
-        assert attn_metadata is not None
-        assert attn_metadata.attn_mask is not None
-
-        if is_310p():
-            # Do reformat in case of broadcasted tensors.
-            attn_metadata.attn_mask = \
-                torch_npu.npu_format_cast(attn_metadata.attn_mask.contiguous(),
-                                          ACL_FORMAT_FRACTAL_NZ)
-            attn_metadata.seq_lens = \
-                attn_metadata.seq_lens.to(device=query.device)
+        if self.sinks is not None:
+            if self.sliding_window is not None:
+                sparse_mode=4
+            else:
+                sparse_mode=3
+            num_block, block_size, _, _ = self.key_cache.shape  # type: ignore
+            output, _ = torch_npu.npu_fused_infer_attention_score_v2(
+                    query,
+                    self.key_cache.view(num_block, block_size, self.num_kv_heads * self.head_size),
+                    self.value_cache.view(num_block, block_size, self.num_kv_heads * self.head_size),
+                    num_query_heads=self.num_heads,
+                    num_key_value_heads=self.num_kv_heads,
+                    input_layout="TND",
+                    pre_tokens=self.sliding_window if self.sliding_window is not None else 2147483647,
+                    next_tokens=0,
+                    atten_mask=self.share_mask_tril_spase,
+                    sparse_mode=sparse_mode,
+                    softmax_scale=self.scale,
+                    block_table=attn_metadata.block_tables,
+                    block_size=block_size,
+                    actual_seq_qlen=attn_metadata.query_start_loc_list,
+                    actual_seq_kvlen=attn_metadata.seq_lens,
+                    learnable_sink=self.sinks,
+                )
+        else:
+            # Use chunked prefill for head size 192 scenario, like deepseek
+            # paged_attention_splitfuse maybe crash at such scenario.
+            # TODO: vanilla path will be removed after the kernel support
+            # head_size 192 scenario.
+            if self.head_size == 192:
+                cu_seqlen_q = [0] + attn_metadata.query_lens.tolist()
+                cu_seqlen_k = [0] + attn_metadata.seq_lens.tolist()
+                cu_seqlen_q = torch.tensor(cu_seqlen_q, device=query.device)
+                cu_seqlen_k = torch.tensor(cu_seqlen_k, device=query.device)
+                cu_seqlen_q = torch.cumsum(cu_seqlen_q, dim=0)
+                cu_seqlen_k = torch.cumsum(cu_seqlen_k, dim=0)
+                max_seqlen_q = torch.max(attn_metadata.query_lens)
+                max_seqlen_k = torch.max(attn_metadata.seq_lens)
+                vanilla_chunked_prefill(output, query, self.key_cache,
+                                        self.value_cache,
+                                        attn_metadata.block_tables, cu_seqlen_q,
+                                        cu_seqlen_k, max_seqlen_q, max_seqlen_k,
+                                        self.scale, None, True)
+                return output
+
+            # Use paged attention.
+            assert attn_metadata is not None
+            assert attn_metadata.attn_mask is not None
+
+            if is_310p():
+                # Do reformat in case of broadcasted tensors.
+                attn_metadata.attn_mask = \
+                    torch_npu.npu_format_cast(attn_metadata.attn_mask.contiguous(),
+                                            ACL_FORMAT_FRACTAL_NZ)
+                attn_metadata.seq_lens = \
+                    attn_metadata.seq_lens.to(device=query.device)
 
-        # TODO:The npu_fused_infer_attention_score op is planned to
-        # be utilized in a wider range in upcoming versions.
-        num_block, block_size, _, _ = self.key_cache.shape  # type: ignore
-        key = self.key_cache.view(  # type: ignore
-            num_block, block_size, -1)
-        value = self.value_cache.view(  # type: ignore
-            num_block, block_size, -1)
+            # TODO:The npu_fused_infer_attention_score op is planned to
+            # be utilized in a wider range in upcoming versions.
+            num_block, block_size, _, _ = self.key_cache.shape  # type: ignore
+            key = self.key_cache.view(  # type: ignore
+                num_block, block_size, -1)
+            value = self.value_cache.view(  # type: ignore
+                num_block, block_size, -1)
 
-        output, _ = torch_npu.npu_fused_infer_attention_score(
-            query=query,
-            key=key,
-            value=value,
-            atten_mask=attn_metadata.attn_mask,
-            block_table=attn_metadata.block_tables,
-            input_layout="TND",
-            block_size=block_size,
-            actual_seq_lengths=attn_metadata.actual_seq_lengths_q,
-            actual_seq_lengths_kv=attn_metadata.seq_lens_list,
-            num_key_value_heads=self.num_kv_heads,
-            num_heads=self.num_heads,
-            scale=self.scale,
-            sparse_mode=3,
-        )
+            output, _ = torch_npu.npu_fused_infer_attention_score(
+                query=query,
+                key=key,
+                value=value,
+                atten_mask=attn_metadata.attn_mask,
+                block_table=attn_metadata.block_tables,
+                input_layout="TND",
+                block_size=block_size,
+                actual_seq_lengths=attn_metadata.actual_seq_lengths_q,
+                actual_seq_lengths_kv=attn_metadata.seq_lens_list,
+                num_key_value_heads=self.num_kv_heads,
+                num_heads=self.num_heads,
+                scale=self.scale,
+                sparse_mode=3,
+            )
         return output
 
     def _attention_with_nomask_and_mask(self, q: torch.Tensor,
