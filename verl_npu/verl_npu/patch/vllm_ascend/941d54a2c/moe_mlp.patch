diff --git a/vllm_ascend/ops/fused_moe/moe_mlp.py b/vllm_ascend/ops/fused_moe/moe_mlp.py
index 0e2b81fb..bb1d93c2 100644
--- a/vllm_ascend/ops/fused_moe/moe_mlp.py
+++ b/vllm_ascend/ops/fused_moe/moe_mlp.py
@@ -22,7 +22,7 @@ from torch.nn.functional import pad
 from vllm.forward_context import get_forward_context
 
 from vllm_ascend.ascend_forward_context import MoECommType
-from vllm_ascend.utils import dispose_tensor, is_310p
+from vllm_ascend.utils import dispose_tensor, is_310p, get_use_sink_attention
 
 
 def cumsum_group_list(group_list: torch.Tensor,
@@ -194,25 +194,45 @@ def unquant_apply_mlp(hidden_states: torch.Tensor,
                       w1: torch.Tensor,
                       w2: torch.Tensor,
                       group_list: torch.Tensor,
+                      w1_bias: Optional[torch.Tensor] = None,
+                      w2_bias: Optional[torch.Tensor] = None,
+                      activation: Optional[str] = None,
                       group_list_type: int = 1,
                       topk_scales: Optional[torch.Tensor] = None,
                       need_trans: bool = True) -> torch.Tensor:
 
-    if need_trans:
+    if need_trans or get_use_sink_attention():
         w1 = w1.transpose(1, 2)
         w2 = w2.transpose(1, 2)
 
     gate_up_out = torch_npu.npu_grouped_matmul(
         x=[hidden_states],
         weight=[w1],
+        bias=[w1_bias.to(
+            dtype=torch.float32)] if w1_bias is not None else None,
         split_item=2,
         group_list_type=group_list_type,
         group_type=0,
         group_list=group_list,
     )[0]
+
+    # Do activation
+    def swiglu_oai(gate_up):
+        alpha = 1.702
+        limit = 7.0
+        gate, up = gate_up[..., ::2], gate_up[..., 1::2]
+        gate = gate.clamp(min=None, max=limit)
+        up = up.clamp(min=-limit, max=limit)
+        glu = gate * torch.sigmoid(gate * alpha)
+        gated_output = (up + 1) * glu
+        return gated_output
+
     if is_310p():
         gate_up_out = torch_npu.npu_swiglu(gate_up_out.to(torch.float32)).to(
             torch.float16)
+    elif activation == "swigluoai":
+        num_experts, _, hidden_size = w1.shape
+        gate_up_out = swiglu_oai(gate_up_out.view(-1, hidden_size))
     else:
         gate_up_out = torch_npu.npu_swiglu(gate_up_out)
 
@@ -222,6 +242,8 @@ def unquant_apply_mlp(hidden_states: torch.Tensor,
     hidden_states = torch_npu.npu_grouped_matmul(
         x=[gate_up_out],
         weight=[w2],
+        bias=[w2_bias.to(
+            dtype=torch.float32)] if w2_bias is not None else None,
         split_item=2,
         group_list_type=group_list_type,
         group_type=0,
@@ -236,6 +258,9 @@ def unified_apply_mlp(hidden_states: torch.Tensor,
                       w2: torch.Tensor,
                       w2_scale: torch.Tensor,
                       group_list: torch.Tensor,
+                      activation: Optional[str] = None,
+                      w1_bias: Optional[torch.Tensor] = None,
+                      w2_bias: Optional[torch.Tensor] = None,
                       dynamic_scale: torch.Tensor = None,
                       group_list_type: int = 1,
                       w1_scale_bias: torch.Tensor = None,
@@ -262,6 +287,9 @@ def unified_apply_mlp(hidden_states: torch.Tensor,
         return unquant_apply_mlp(hidden_states=hidden_states,
                                  w1=w1,
                                  w2=w2,
+                                 w1_bias=w1_bias,
+                                 w2_bias=w2_bias,
+                                 activation=activation,
                                  group_list=group_list,
                                  group_list_type=group_list_type,
                                  topk_scales=topk_scales,
